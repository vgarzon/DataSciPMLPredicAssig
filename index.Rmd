---
title: "Practical Machine Learning Course Project"
subtitle: "Subtitle"
author: "Victor Garzon"
date: "March 30, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache=TRUE)
```

## Summary

## Setup

We download the files to a local folder and read them, treating the string `#DIV/0!` as `NA`.

```{r}
url_prefix  <- "https://d396qusza40orc.cloudfront.net/predmachlearn/"
file_names <- c(train = "pml-training.csv", test = "pml-testing.csv")
data_dir <- "./data/"
# Download data files if needed
if ( !dir.exists(data_dir) ) dir.create(data_dir)
for (fn in file_names) {
    dst <- paste0(data_dir, fn)
    if ( !file.exists(dst) )
        download.file(paste0(url_prefix, fn), dst)
}
# Read data from files, treat "#DIV/0!" as NA
pml_data <- read.csv(paste0(data_dir, file_names["train"]), 
                      na.strings = c("NA", "#DIV/0!"))
pml_test  <- read.csv(paste0(data_dir, file_names["test"]))
```

The training data file has `r nrow(pml_data)` rows and `r ncol(pml_data)` columns.  To train a model we select a smaller subset of rows selected at random.
```{r}
n_small <- 5000
pml_train <- pml_data
n_obs <- nrow(pml_train)
if (n_small > 0 && n_small <= n_obs) {
    set.seed(847162)
    pml_train <- pml_train[sample(1:n_obs, n_small), ]
}
```

## Data Exploration

We exclude variables that are mostly `NA`, with a threshold of 90%.

```{r}
frac_na <- apply(apply(pml_train, 2, is.na), 2, mean)
ind_col <- which(frac_na < 0.90)
```

Before training a model we look at some of the variables, starting with `user_names` and `new_window`.

```{r}
round(prop.table(table(pml_train$user_name, pml_train$classe), 2), 3)
plot(table(pml_train$classe, pml_train$user_name))
round(prop.table(table(pml_train$new_window, pml_train$classe), 2), 3)
```

There is no direct link between `user_names` and `classe`.  The factor variable `new_window` has mostly value of `no`.  Hence we exclude both variables from the model.  We're not concerned with time-dependent trends in the data, so we we exclude time-stamp variables.

```{r}
pml_names <- names(pml_train)
ind_excl <- match(c("X", "user_name", "new_window"), pml_names)
ind_excl <- c(ind_excl, grep("timestamp", pml_names))
```

Highly-correlated features are excluded, with a correlation threshold of 80%.  

```{r}
library(caret, warn.conflicts = FALSE)
ind_col2 <- setdiff(ind_col, ind_excl)
cor_train <- cor(pml_train[, ind_col2[1:length(ind_col2)-1]])
cor_vars <- findCorrelation(cor_train, cutoff = 0.8, names = TRUE)
ind_train <- setdiff(ind_col2, match(cor_vars, pml_names))
```

The number of variables has been reduced to `r length(ind_train)`.  The data is split into training and testing sets with a ratio of 7:3.

```{r}
set.seed(39847)
inTrain <- createDataPartition( y = pml_train$classe, p=0.7, list=FALSE)
training <- pml_train[ inTrain, ind_train]
testing  <- pml_train[-inTrain, ind_train]
dim(training); dim(testing)
```

## Model Training

```{r}
qplot(num_window, classe, data = training)
```

Hence we expect variable `num_window` is a very good predictor for `classe`.  To test this we train a $k$-nearest neighbors model using `num_window` as the sole predictor.

```{r}
mod2 <- train(classe ~ num_window, method = "knn", data = training)
mod2
```

As an alternative we fit a **random forest** model with 50 trees excluding predictor `num_window`. We use **cross-validation** to tune the parameter `mtry`, the maximum number of randomly selected predictors.  We try `mtry` values from 2 to `r ncol(training)-2`.

```{r}
n_tree = 50
ctr1 <- trainControl(method = "repeatedcv", number = 10, repeats = 10)
tgd1 <- expand.grid(mtry = c(2, 5, 10, 20, 30, ncol(training)-2))
mod1 <- train(classe ~ . -num_window, method = "rf", 
              data = training, ntree = n_tree, 
              trControl = ctr1, tuneGrid = tgd1)
```

Summary of **Accuracy** for 10x10 resamples shows estimated out-of-sample accuracy of **95.4**.
```{r}
mod1
plot(mod1)
summary(mod1$resample$Accuracy)
```

We test the model on the 30% of data set aside, confirming the estimation of out-of-sample prediction error.
```{r}
cm1 <- confusionMatrix(predict(mod1, testing), testing$classe)
cm1$table
cm1$overall["Accuracy"]
```

Finally we predict the quiz values
```{r}
predict(mod1, pml_test)
```

## Conclusions


## Appendix

Plot of 20 most important variables by mean decrease of Gini statistic.
```{r}
varImpPlot(mod1$finalModel, n.var = 20)
```

Plot of model error as a function of number of trees
```{r}
plot(mod1$finalModel, type = "l")
```

